{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6514072b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)  [(None, 28, 28, 1)]          0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 28, 28, 32)           320       ['encoder_input[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 64)           18496     ['conv2d[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 14, 14, 64)           36928     ['conv2d_1[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 14, 14, 64)           36928     ['conv2d_2[0][0]']            \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 12544)                0         ['conv2d_3[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 32)                   401440    ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " latent_mu (Dense)           (None, 2)                    66        ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " latent_sigma (Dense)        (None, 2)                    66        ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " z (Lambda)                  (None, 2)                    0         ['latent_mu[0][0]',           \n",
      "                                                                     'latent_sigma[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 494244 (1.89 MB)\n",
      "Trainable params: 494244 (1.89 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 12544)             37632     \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTr  (None, 28, 28, 32)        18464     \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " decoder_output (Conv2DTran  (None, 28, 28, 1)         289       \n",
      " spose)                                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 56385 (220.25 KB)\n",
      "Trainable params: 56385 (220.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"vae\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)  [(None, 28, 28, 1)]          0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 28, 28, 32)           320       ['encoder_input[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 64)           18496     ['conv2d[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 14, 14, 64)           36928     ['conv2d_1[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 14, 14, 64)           36928     ['conv2d_2[0][0]']            \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 12544)                0         ['conv2d_3[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 32)                   401440    ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " latent_mu (Dense)           (None, 2)                    66        ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " latent_sigma (Dense)        (None, 2)                    66        ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " z (Lambda)                  (None, 2)                    0         ['latent_mu[0][0]',           \n",
      "                                                                     'latent_sigma[0][0]']        \n",
      "                                                                                                  \n",
      " decoder (Functional)        (None, 28, 28, 1)            56385     ['z[0][0]']                   \n",
      "                                                                                                  \n",
      " custom_layer (CustomLayer)  (None, 28, 28, 1)            0         ['encoder_input[0][0]',       \n",
      "                                                                     'decoder[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 550629 (2.10 MB)\n",
      "Trainable params: 550629 (2.10 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/Users/ajayramaraju/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/ajayramaraju/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/ajayramaraju/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/ajayramaraju/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1127, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/ajayramaraju/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1185, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/ajayramaraju/anaconda3/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 329, in __call__\n        self._total_loss_mean.update_state(\n    File \"/Users/ajayramaraju/anaconda3/lib/python3.11/site-packages/keras/src/utils/metrics_utils.py\", line 77, in decorated\n        result = update_state_fn(*args, **kwargs)\n    File \"/Users/ajayramaraju/anaconda3/lib/python3.11/site-packages/keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/Users/ajayramaraju/anaconda3/lib/python3.11/site-packages/keras/src/metrics/base_metric.py\", line 528, in update_state  **\n        update_total_op = self.total.assign_add(value_sum)\n    File \"/Users/ajayramaraju/anaconda3/lib/python3.11/site-packages/keras/src/engine/keras_tensor.py\", line 285, in __array__\n        raise TypeError(\n\n    TypeError: You are passing KerasTensor(type_spec=TensorSpec(shape=(), dtype=tf.float32, name=None), name='tf.math.reduce_sum/Sum:0', description=\"created by layer 'tf.math.reduce_sum'\"), an intermediate Keras symbolic input/output, to a TF API that does not allow registering custom dispatchers, such as `tf.cond`, `tf.function`, gradient tapes, or `tf.map_fn`. Keras Functional model construction only supports TF API calls that *do* support dispatching, such as `tf.math.add` or `tf.reshape`. Other APIs cannot be called directly on symbolic Kerasinputs/outputs. You can work around this limitation by putting the operation in a custom Keras layer `call` and calling that layer on this symbolic input/output.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 172\u001b[0m\n\u001b[1;32m    169\u001b[0m vae\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# Train autoencoder\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m vae\u001b[38;5;241m.\u001b[39mfit(x_train, \u001b[38;5;28;01mNone\u001b[39;00m, epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m, validation_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# =================\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Visualize results\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# =================\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m#Visualize inputs mapped to the Latent space\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m#Remember that we have encoded inputs to latent space dimension = 2. \u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m#Extract z_mu --> first parameter in the result of encoder prediction representing mean\u001b[39;00m\n\u001b[1;32m    181\u001b[0m mu, _, _ \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mpredict(x_test)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/j8/hz_1dvqs6fv9xcs7x401v4gh0000gn/T/__autograph_generated_filewgt934ke.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/Users/ajayramaraju/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/ajayramaraju/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/ajayramaraju/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/ajayramaraju/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1127, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/ajayramaraju/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1185, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/ajayramaraju/anaconda3/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 329, in __call__\n        self._total_loss_mean.update_state(\n    File \"/Users/ajayramaraju/anaconda3/lib/python3.11/site-packages/keras/src/utils/metrics_utils.py\", line 77, in decorated\n        result = update_state_fn(*args, **kwargs)\n    File \"/Users/ajayramaraju/anaconda3/lib/python3.11/site-packages/keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/Users/ajayramaraju/anaconda3/lib/python3.11/site-packages/keras/src/metrics/base_metric.py\", line 528, in update_state  **\n        update_total_op = self.total.assign_add(value_sum)\n    File \"/Users/ajayramaraju/anaconda3/lib/python3.11/site-packages/keras/src/engine/keras_tensor.py\", line 285, in __array__\n        raise TypeError(\n\n    TypeError: You are passing KerasTensor(type_spec=TensorSpec(shape=(), dtype=tf.float32, name=None), name='tf.math.reduce_sum/Sum:0', description=\"created by layer 'tf.math.reduce_sum'\"), an intermediate Keras symbolic input/output, to a TF API that does not allow registering custom dispatchers, such as `tf.cond`, `tf.function`, gradient tapes, or `tf.map_fn`. Keras Functional model construction only supports TF API calls that *do* support dispatching, such as `tf.math.add` or `tf.reshape`. Other APIs cannot be called directly on symbolic Kerasinputs/outputs. You can work around this limitation by putting the operation in a custom Keras layer `call` and calling that layer on this symbolic input/output.\n"
     ]
    }
   ],
   "source": [
    "#https://youtu.be/YV9D3TWY5Zo\n",
    "#https://youtu.be/8wrLjnQ7EWQ\n",
    "\n",
    "\"\"\"\n",
    "VAEs can be used for generative purposes. \n",
    "\n",
    "This code demonstrates VAE using MNIST dataset.\n",
    "Just like regular autoencoder VAE returns an array (image) of same domensions\n",
    "as input but we can introduce variation by tweaking the latent vector.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
    "#from keras.layers import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "#Normalize and reshape ============\n",
    "\n",
    "#Norm.\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# Reshape \n",
    "img_width  = x_train.shape[1]\n",
    "img_height = x_train.shape[2]\n",
    "num_channels = 1 #MNIST --> grey scale so 1 channel\n",
    "x_train = x_train.reshape(x_train.shape[0], img_height, img_width, num_channels)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_height, img_width, num_channels)\n",
    "input_shape = (img_height, img_width, num_channels)\n",
    "# ========================\n",
    "#View a few images\n",
    "plt.figure(1)\n",
    "plt.subplot(221)\n",
    "plt.imshow(x_train[42][:,:,0])\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.imshow(x_train[420][:,:,0])\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.imshow(x_train[4200][:,:,0])\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.imshow(x_train[42000][:,:,0])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# BUILD THE MODEL\n",
    "\n",
    "# # ================= #############\n",
    "# # Encoder\n",
    "#Let us define 4 conv2D, flatten and then dense\n",
    "# # ================= ############\n",
    "\n",
    "latent_dim = 2 # Number of latent dim parameters\n",
    "\n",
    "input_img = Input(shape=input_shape, name='encoder_input')\n",
    "x = Conv2D(32, 3, padding='same', activation='relu')(input_img)\n",
    "x = Conv2D(64, 3, padding='same', activation='relu',strides=(2, 2))(x)\n",
    "x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "\n",
    "conv_shape = K.int_shape(x) #Shape of conv to be provided to decoder\n",
    "#Flatten\n",
    "x = Flatten()(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "\n",
    "# Two outputs, for latent mean and log variance (std. dev.)\n",
    "#Use these to sample random variables in latent space to which inputs are mapped. \n",
    "z_mu = Dense(latent_dim, name='latent_mu')(x)   #Mean values of encoded input\n",
    "z_sigma = Dense(latent_dim, name='latent_sigma')(x)  #Std dev. (variance) of encoded input\n",
    "\n",
    "#REPARAMETERIZATION TRICK\n",
    "# Define sampling function to sample from the distribution\n",
    "# Reparameterize sample based on the process defined by Gunderson and Huang\n",
    "# into the shape of: mu + sigma squared x eps\n",
    "#This is to allow gradient descent to allow for gradient estimation accurately. \n",
    "def sample_z(args):\n",
    "  z_mu, z_sigma = args\n",
    "  eps = K.random_normal(shape=(K.shape(z_mu)[0], K.int_shape(z_mu)[1]))\n",
    "  return z_mu + K.exp(z_sigma / 2) * eps\n",
    "\n",
    "# sample vector from the latent distribution\n",
    "# z is the labda custom layer we are adding for gradient descent calculations\n",
    "  # using mu and variance (sigma)\n",
    "z = Lambda(sample_z, output_shape=(latent_dim, ), name='z')([z_mu, z_sigma])\n",
    "\n",
    "#Z (lambda layer) will be the last layer in the encoder.\n",
    "# Define and summarize encoder model.\n",
    "encoder = Model(input_img, [z_mu, z_sigma, z], name='encoder')\n",
    "print(encoder.summary())\n",
    "\n",
    "# ================= ###########\n",
    "# Decoder\n",
    "#\n",
    "# ================= #################\n",
    "\n",
    "# decoder takes the latent vector as input\n",
    "decoder_input = Input(shape=(latent_dim, ), name='decoder_input')\n",
    "\n",
    "# Need to start with a shape that can be remapped to original image shape as\n",
    "#we want our final utput to be same shape original input.\n",
    "#So, add dense layer with dimensions that can be reshaped to desired output shape\n",
    "x = Dense(conv_shape[1]*conv_shape[2]*conv_shape[3], activation='relu')(decoder_input)\n",
    "# reshape to the shape of last conv. layer in the encoder, so we can \n",
    "x = Reshape((conv_shape[1], conv_shape[2], conv_shape[3]))(x)\n",
    "# upscale (conv2D transpose) back to original shape\n",
    "# use Conv2DTranspose to reverse the conv layers defined in the encoder\n",
    "x = Conv2DTranspose(32, 3, padding='same', activation='relu',strides=(2, 2))(x)\n",
    "#Can add more conv2DTranspose layers, if desired. \n",
    "#Using sigmoid activation\n",
    "x = Conv2DTranspose(num_channels, 3, padding='same', activation='sigmoid', name='decoder_output')(x)\n",
    "\n",
    "# Define and summarize decoder model\n",
    "decoder = Model(decoder_input, x, name='decoder')\n",
    "decoder.summary()\n",
    "\n",
    "# apply the decoder to the latent sample \n",
    "z_decoded = decoder(z)\n",
    "\n",
    "\n",
    "# =========================\n",
    "#Define custom loss\n",
    "#VAE is trained using two loss functions reconstruction loss and KL divergence\n",
    "#Let us add a class to define a custom layer with loss\n",
    "class CustomLayer(keras.layers.Layer):\n",
    "\n",
    "    def vae_loss(self, x, z_decoded):\n",
    "        x = K.flatten(x)\n",
    "        z_decoded = K.flatten(z_decoded)\n",
    "        \n",
    "        # Reconstruction loss (as we used sigmoid activation we can use binarycrossentropy)\n",
    "        recon_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n",
    "        \n",
    "        # KL divergence\n",
    "        kl_loss = -5e-4 * K.mean(1 + z_sigma - K.square(z_mu) - K.exp(z_sigma), axis=-1)\n",
    "        return K.mean(recon_loss + kl_loss)\n",
    "\n",
    "    # add custom loss to the class\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        z_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, z_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return x\n",
    "\n",
    "# apply the custom loss to the input images and the decoded latent distribution sample\n",
    "y = CustomLayer()([input_img, z_decoded])\n",
    "# y is basically the original image after encoding input img to mu, sigma, z\n",
    "# and decoding sampled z values.\n",
    "#This will be used as output for vae\n",
    "\n",
    "# =================\n",
    "# VAE \n",
    "# =================\n",
    "vae = Model(input_img, y, name='vae')\n",
    "\n",
    "# Compile VAE\n",
    "vae.compile(optimizer='adam', loss=None)\n",
    "vae.summary()\n",
    "\n",
    "# Train autoencoder\n",
    "vae.fit(x_train, None, epochs = 10, batch_size = 32, validation_split = 0.2)\n",
    "\n",
    "# =================\n",
    "# Visualize results\n",
    "# =================\n",
    "#Visualize inputs mapped to the Latent space\n",
    "#Remember that we have encoded inputs to latent space dimension = 2. \n",
    "#Extract z_mu --> first parameter in the result of encoder prediction representing mean\n",
    "\n",
    "mu, _, _ = encoder.predict(x_test)\n",
    "#Plot dim1 and dim2 for mu\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(mu[:, 0], mu[:, 1], c=y_test, cmap='brg')\n",
    "plt.xlabel('dim 1')\n",
    "plt.ylabel('dim 2')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Visualize images\n",
    "#Single decoded image with random input latent vector (of size 1x2)\n",
    "#Latent space range is about -5 to 5 so pick random values within this range\n",
    "#Try starting with -1, 1 and slowly go up to -1.5,1.5 and see how it morphs from \n",
    "#one image to the other.\n",
    "sample_vector = np.array([[1,-1]])\n",
    "decoded_example = decoder.predict(sample_vector)\n",
    "decoded_example_reshaped = decoded_example.reshape(img_width, img_height)\n",
    "plt.imshow(decoded_example_reshaped)\n",
    "\n",
    "#Let us automate this process by generating multiple images and plotting\n",
    "#Use decoder to generate images by tweaking latent variables from the latent space\n",
    "#Create a grid of defined size with zeros. \n",
    "#Take sample from some defined linear space. In this example range [-4, 4]\n",
    "#Feed it to the decoder and update zeros in the figure with output.\n",
    "\n",
    "\n",
    "n = 20  # generate 15x15 digits\n",
    "figure = np.zeros((img_width * n, img_height * n, num_channels))\n",
    "\n",
    "#Create a Grid of latent variables, to be provided as inputs to decoder.predict\n",
    "#Creating vectors within range -5 to 5 as that seems to be the range in latent space\n",
    "grid_x = np.linspace(-5, 5, n)\n",
    "grid_y = np.linspace(-5, 5, n)[::-1]\n",
    "\n",
    "# decoder for each square in the grid\n",
    "for i, yi in enumerate(grid_y):\n",
    "    for j, xi in enumerate(grid_x):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        x_decoded = decoder.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(img_width, img_height, num_channels)\n",
    "        figure[i * img_width: (i + 1) * img_width,\n",
    "               j * img_height: (j + 1) * img_height] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "#Reshape for visualization\n",
    "fig_shape = np.shape(figure)\n",
    "figure = figure.reshape((fig_shape[0], fig_shape[1]))\n",
    "\n",
    "plt.imshow(figure, cmap='gnuplot2')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5533a456",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
